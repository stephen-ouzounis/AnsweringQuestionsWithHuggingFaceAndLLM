{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b11b26db-2ea6-4280-a648-52d0c5d96e7b",
   "metadata": {},
   "source": [
    "# 3.1 Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd387dc-2113-43f7-b279-ccafbe14fda1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pip installation LangChain and Hugginface API\n",
    "!pip install langchain\n",
    "!pip install huggingface_hub\n",
    "\n",
    "# Pip installation of additional needed libraries\n",
    "!pip install sentence_transformers\n",
    "!pip install faiss-cpu\n",
    "!pip install unstructured\n",
    "\n",
    "# To download the transcript of a youtube video\n",
    "!pip install youtube_transcript_api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3409a776-5bb6-4f40-9751-d316649c89ae",
   "metadata": {},
   "source": [
    "# 3.2 Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ed8cfad-8e90-436e-a2d3-49b675363129",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"YOUR API TOKEN\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b05718-d806-496e-88c2-4303d2ebff63",
   "metadata": {},
   "source": [
    "# 4.1 Loading of documents as a learning basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "945f07ea-7b4a-4abc-9618-5ec798e94e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0879212a-ab69-4df6-b124-fab0d6a72ce2",
   "metadata": {},
   "source": [
    "## 4.1.1 TextLoader from Local & GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a20ea47c-5005-435c-9f18-828a29080b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTXTFileFromLocal(local_file_name=\"local_text_file.txt\"):\n",
    "    # Load the text data\n",
    "    with open('./'+local_file_name, \"r\", encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        \n",
    "    with open('./'+local_file_name, \"w\",  encoding='utf-8') as file:\n",
    "      file.write(text)\n",
    "\n",
    "    # Load the text document using TextLoader\n",
    "    loader = TextLoader('./'+local_file_name)\n",
    "    loaded_docs = loader.load()\n",
    "    return loaded_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccecbc6-8bca-4c0b-b75c-2a63b1caf359",
   "metadata": {},
   "source": [
    "## 4.1.2 TextLoader from URL (GitHub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "915dd66f-75bb-4c77-b26a-74773465a200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTXTFileFromURL(text_file_url=\"https://raw.githubusercontent.com/vashAI/AnsweringQuestionsWithHuggingFaceAndLLM/main/url_text_file.txt\"):\n",
    "    # Fetching the text file\n",
    "    output_file_name = \"url_text_file.txt\"\n",
    "    response = requests.get(text_file_url)\n",
    "    with open(output_file_name, \"w\",  encoding='utf-8') as file:\n",
    "      file.write(response.text)\n",
    "\n",
    "    # Load the text document using TextLoader\n",
    "    loader = TextLoader('./'+output_file_name)\n",
    "    loaded_docs = loader.load()\n",
    "    return loaded_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090cedd5-54ce-4750-93d9-4e5482d4a33d",
   "metadata": {},
   "source": [
    "## 4.1.3 PDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8eb936a-e1fd-47e7-82be-c51964ef5c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredPDFLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5805a745-50fb-4513-b66e-49573649ce4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadPDFFromLocal(pdf_file_path=\"./Eurovision_Song_Contest_2023.pdf\"):\n",
    "    loader = UnstructuredPDFLoader(pdf_file_path)\n",
    "    loaded_docs = loader.load()\n",
    "    return loaded_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28397ce0-d1ef-49d0-85fd-918665439267",
   "metadata": {},
   "source": [
    "## 4.1.4 WebsiteLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da07186d-e29c-4890-870f-fe0109d0119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredURLLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed8976b4-4e69-4f55-b3eb-c3db943b3d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTextFromWebsite(url=\"https://saturncloud.io/blog/breaking-the-data-barrier-how-zero-shot-one-shot-and-few-shot-learning-are-transforming-machine-learning/\"):\n",
    "    loader = UnstructuredURLLoader(urls=[url])\n",
    "    loaded_docs = loader.load()\n",
    "    return loaded_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07c15cf-1574-4441-9391-0f26b8ede1e2",
   "metadata": {},
   "source": [
    "## 4.1.5 VideoLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab058904-b69f-4916-9d84-7879c6007049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "922e4082-c2a3-4883-8a49-d27767bbc54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTextFromYoutubeVideo(youtube_video_id=\"eg9qDjws_bU\"):\n",
    "    transcript = YouTubeTranscriptApi.get_transcript(youtube_video_id)\n",
    "\n",
    "    transcript_text = \"\"\n",
    "    for entry in transcript:\n",
    "        transcript_text += ' ' + entry['text']\n",
    "    \n",
    "    youtube_local_txt_file = \"youtube_transcript.txt\"\n",
    "    with open('./'+youtube_local_txt_file, \"w\",  encoding='utf-8') as file:\n",
    "      file.write(transcript_text)\n",
    "\n",
    "    # Load the text document using TextLoader\n",
    "    loader = TextLoader('./'+youtube_local_txt_file)\n",
    "    loaded_docs = loader.load()\n",
    "    return loaded_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713c7310-c185-42ba-8c69-de4a55ebdd20",
   "metadata": {},
   "source": [
    "# 4.2 Split the documents in chunks (Important as LLM cannot accept too long inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c49ef93a-b00c-4466-8e41-21ea27504079",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa9aef4a-9bb4-49e4-97ce-9eb3ea356290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDocument(loaded_docs):\n",
    "    # Splitting documents into chunks\n",
    "    splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=10)\n",
    "    chunked_docs = splitter.split_documents(loaded_docs)\n",
    "    return chunked_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f333094-c0bb-4117-910c-fa3dc16f7eb7",
   "metadata": {},
   "source": [
    "# 4.3 Convert the documents into embeddings and store them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "781fc65f-5e72-4842-b30d-c74c5b806f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd057fc0-70b0-4ef6-8018-6136fe6c8dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEmbeddings(chunked_docs):\n",
    "    # Create embeddings and store them in a FAISS vector store\n",
    "    embedder = HuggingFaceEmbeddings()\n",
    "    vector_store = FAISS.from_documents(chunked_docs, embedder)\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37754d9c-ee8c-4d08-82b7-846202e75cba",
   "metadata": {},
   "source": [
    "# 4.4 Use those embeddings to feed the LLM model and Answer Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "560479d2-d656-46cc-b672-9393d3d108be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain import HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e492b83-9d1e-48dc-abc2-86638f1f9a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadLLMModel():\n",
    "    llm=HuggingFaceHub(repo_id=\"declare-lab/flan-alpaca-large\", model_kwargs={\"temperature\":0, \"max_length\":512})\n",
    "    chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "    return chain\n",
    "\n",
    "def askQuestions(vector_store, chain, question):\n",
    "    # Ask a question using the QA chain\n",
    "    similar_docs = vector_store.similarity_search(question)\n",
    "    response = chain.run(input_documents=similar_docs, question=question)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "861d5dc9-3345-41b5-ab89-fa5110a84a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = loadLLMModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21adbcda-48d7-4404-9812-08aa7985fc0a",
   "metadata": {},
   "source": [
    "## 4.4.1 Test with Local file & Test with file from URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05458723-fd52-4901-90b3-4be88efc7f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_loaded_docs = loadTXTFileFromLocal()\n",
    "LOCAL_chunked_docs = splitDocument(LOCAL_loaded_docs)\n",
    "LOCAL_vector_store = createEmbeddings(LOCAL_chunked_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1cf93661-aafd-40b6-913c-7fc273fa22f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGPT and plugins are helping Citizen Data Scientists by providing them with the tools they need to analyze and interpret data. By enabling them to use natural language, they are able to ask questions and get answers in plain English, without knowing complex programming languages or statistical techniques. Additionally, ChatGPT is a personal expert who is always available to help them turn their idea into reality.\n"
     ]
    }
   ],
   "source": [
    "LOCAL_response = askQuestions(LOCAL_vector_store, chain, \"Explain me how ChatGPT and Plugin are empowering Citizen Data Scientists?\")\n",
    "print(LOCAL_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0eae1e-0fb4-4462-85e6-d8d963e3b12d",
   "metadata": {},
   "source": [
    "## 4.4.2 Test with file from URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "513281b6-9849-4776-bcb1-eae6cdcc4a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_loaded_docs = loadTXTFileFromURL()\n",
    "URL_chunked_docs = splitDocument(URL_loaded_docs)\n",
    "URL_vector_store = createEmbeddings(URL_chunked_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da9f68b3-4b26-463f-90de-3615445cf7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Data visualization and analysis using ChatGPT and Plugins 2. Content creation and summarization 3. Personalized learning and skill development 4. Collaboration and knowledge sharing\n"
     ]
    }
   ],
   "source": [
    "URL_response = askQuestions(URL_vector_store, chain, \"What are 5 examples of chatgpt and plugin applications?\")\n",
    "print(URL_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c417bb78-9288-4b4b-9508-4ca5d3e9118d",
   "metadata": {},
   "source": [
    "## 4.4.3 Test with PDF from local path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8d35796-05d7-45d8-9736-3134c78adb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_loaded_docs = loadPDFFromLocal()\n",
    "PDF_chunked_docs = splitDocument(PDF_loaded_docs)\n",
    "PDF_vector_store = createEmbeddings(PDF_chunked_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14452aec-f775-449b-b245-826025e0de71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sweden is the winner of 2023 Eurovision Songcontest.\n"
     ]
    }
   ],
   "source": [
    "PDF_response = askQuestions(PDF_vector_store, chain, \"Who is the Winner of 2023 Eurovision Songcontest?\")\n",
    "print(PDF_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f426c1-b5fc-4348-9516-9d9923a3832d",
   "metadata": {},
   "source": [
    "## 4.4.4 Test with WEBSITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e8d9a41-b15d-4a9b-aad5-de18a7815007",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEBSITE_loaded_docs = loadTextFromWebsite()\n",
    "WEBSITE_chunked_docs = splitDocument(WEBSITE_loaded_docs)\n",
    "WEBSITE_vector_store = createEmbeddings(WEBSITE_chunked_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c10d983-72b9-4c99-a473-5fb840d23993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-Shot Learning is the concept of training a model to classify objects it has never seen before. The core idea is to exploit the existing knowledge of another model to obtain meaningful representations of new classes.\n"
     ]
    }
   ],
   "source": [
    "WEBSITE_response = askQuestions(WEBSITE_vector_store, chain, \"What is Zero-shot learning?\")\n",
    "print(WEBSITE_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96c417c-cc60-4e71-ac34-ba2183c0a370",
   "metadata": {},
   "source": [
    "## 4.4.5 Test with text from video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd000306-fc8d-48f2-9fbd-f1b0674041b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_loaded_docs = loadTextFromYoutubeVideo()\n",
    "VIDEO_chunked_docs = splitDocument(VIDEO_loaded_docs)\n",
    "VIDEO_vector_store = createEmbeddings(VIDEO_chunked_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d264c4fe-5033-4935-bec6-197aca180ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon was explaining that AI is much more dangerous than nuclear warheads and that it could be used to make incredibly effective propaganda.\n"
     ]
    }
   ],
   "source": [
    "VIDEO_response = askQuestions(VIDEO_vector_store, chain, \"What was Elon explaining in the video?\")\n",
    "print(VIDEO_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
